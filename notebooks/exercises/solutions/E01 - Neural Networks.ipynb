{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solid-harris",
   "metadata": {
    "id": "solid-harris"
   },
   "source": [
    "![Logo Uni KÃ¶ln](https://raw.githubusercontent.com/jmelsbach/ai-im/main/img/uni-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-offer",
   "metadata": {
    "id": "bright-offer"
   },
   "source": [
    "# Exercise 01 Notebook -  Image Classification with a Neural Network\n",
    "\n",
    "In this exercise you will implement a neural network from scratch using pytorch. We train the model on the fashion MNIST dataset. The Goal is to find a good architecture and hyperparameters to achieve the highest test score possible!\n",
    "\n",
    "Here are the exact steps we need to take:\n",
    "1. Download the dataset and create a dataset for training, validation and test data.\n",
    "2. Explore and understand the dataset.\n",
    "3. Create DataLoader for training, validation and test set.\n",
    "4. Create a Neural Network Architecture that fits the problem.\n",
    "5. Set the hyperparameters and choose a suitable loss function.\n",
    "6. Create a training loop\n",
    "7. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-panama",
   "metadata": {
    "id": "multiple-panama"
   },
   "source": [
    "We'll use the Fashion-MNIST data set during this exercise. The Fashion-MNIST dataset consists of 60,000 training examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 different classes:\n",
    "\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-bacteria",
   "metadata": {
    "id": "turkish-bacteria"
   },
   "source": [
    "![Fashion MNIST Long](https://raw.githubusercontent.com/jmelsbach/ai-im/main/img/fashion-mnist_long.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-timber",
   "metadata": {
    "id": "respiratory-timber"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library for printing out progress bars\n",
    "# This can be useful in the training loop\n",
    "# The use is optional\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-palmer",
   "metadata": {
    "id": "assisted-palmer"
   },
   "source": [
    "## 1. Download the dataset and create a dataset for training, validation and test data.\n",
    "You can learn how to download the training dataset [here](https://pytorch.org/vision/stable/datasets.html#fashion-mnist). Use the documentation to download the training and test set of the Fashion-MNIST Dataset. Out of the box the images of the dataset have the PIL format but we need them as `torch.Tensors` to feed them in our neural network later on. <br>\n",
    "**Hint**: Use a transform to convert the PIL to the Tensor format. You can learn about transformations [here](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-berkeley",
   "metadata": {
    "id": "peripheral-berkeley"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST('data', download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-chambers",
   "metadata": {
    "id": "pressed-chambers"
   },
   "source": [
    "## 2. Explore and understand the dataset.\n",
    "If you successfully created the dataset objects, try to explore the data.\n",
    "Answer the following questions:\n",
    "* How many training examples do we have?\n",
    "* How many test examples do we have?\n",
    "* What type of datastructure is each datapoint?\n",
    "* Get the shape of the a training image. What does each dimensions mean?\n",
    "    * You notice that the shape is a little bit awkward. We'll deal with this later in the `forward()` method of our neural network\n",
    "* Do we need to normalize the data?\n",
    "* Plot a random image and the corresponding label from the dataset with the help of the `matplotlib`library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-focus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713953828775,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "incomplete-focus",
    "outputId": "7ad12710-2aab-4f0a-ffe4-fbf6898df8ce"
   },
   "outputs": [],
   "source": [
    "# Number of training and test examples\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GxpyZF3bP7QK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713953830286,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "GxpyZF3bP7QK",
    "outputId": "ae709f0d-8e62-4022-fa65-e760e79496b0"
   },
   "outputs": [],
   "source": [
    "# get type of training example\n",
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4O47q24CfVM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713953847896,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "j4O47q24CfVM",
    "outputId": "c91f3d6a-1e91-402b-de98-f709e4166c2d"
   },
   "outputs": [],
   "source": [
    "type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zDA_MeEZP_YV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713953370517,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "zDA_MeEZP_YV",
    "outputId": "f3cda20e-59b8-4190-fcf6-002b2a12fdb6"
   },
   "outputs": [],
   "source": [
    "# image\n",
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auUvHMT3QWKr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713953371945,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "auUvHMT3QWKr",
    "outputId": "dff0e4bd-931b-4d0a-e428-094a20a31367"
   },
   "outputs": [],
   "source": [
    "# label\n",
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mqvrE51lQluk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713953372366,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "mqvrE51lQluk",
    "outputId": "d92def9c-6c30-41f1-f682-d6dfa0bafebd"
   },
   "outputs": [],
   "source": [
    "# Do we need to normalize?\n",
    "train_data[0][0].max(), train_data[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UnCoOgn8RED9",
   "metadata": {
    "id": "UnCoOgn8RED9"
   },
   "outputs": [],
   "source": [
    "# Plot random image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10vBXysTRwNL",
   "metadata": {
    "id": "10vBXysTRwNL"
   },
   "outputs": [],
   "source": [
    "random_index = torch.randint(len(train_data), size=(1,)).item()\n",
    "# or\n",
    "random_index = torch.randint(len(train_data), size=(1,))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uOFWGd_3Q3Uh",
   "metadata": {
    "id": "uOFWGd_3Q3Uh"
   },
   "outputs": [],
   "source": [
    "img = train_data[random_index][0].numpy()\n",
    "label = train_data[random_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1-mnvhtRLNF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1651701166030,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "E1-mnvhtRLNF",
    "outputId": "20b4d345-9eaa-43dd-b14f-8a973f271cf5"
   },
   "outputs": [],
   "source": [
    "print(label)\n",
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JMTD560SQ4ho",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651701166407,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "JMTD560SQ4ho",
    "outputId": "d982e661-9edb-4c6c-e366-936226571459"
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3,3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-toner",
   "metadata": {
    "id": "resident-toner"
   },
   "source": [
    "## 3. Create DataLoader for training, validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-ticket",
   "metadata": {
    "id": "imposed-ticket"
   },
   "source": [
    "We do not have a validation set, yet. Split the `train_data` with the help of the `random_split`. Look at the documentation of the random_split function [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split). Split the data in a 80:20 train/val ratio.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui0HLdS_S200",
   "metadata": {
    "id": "Ui0HLdS_S200"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-bench",
   "metadata": {
    "id": "rotary-bench"
   },
   "outputs": [],
   "source": [
    "train, val = random_split(train_data, [48_000, 12_000],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-encyclopedia",
   "metadata": {
    "id": "verbal-encyclopedia"
   },
   "source": [
    "* Create a `torch.utils.data.DataLoader` for train, val and test data.\n",
    "* Use a batch size of 32.\n",
    "* Don't forget to shuffle the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-fever",
   "metadata": {
    "id": "electoral-fever"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train, batch_size=32, shuffle = True)\n",
    "val_dl   = DataLoader(val, batch_size=32, shuffle=False)\n",
    "test_dl  = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-surname",
   "metadata": {
    "id": "parallel-surname"
   },
   "source": [
    "## 4. Create a Neural Network Architecture that fits the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-price",
   "metadata": {
    "id": "technological-price"
   },
   "source": [
    "Create a Neural Network with two hidden layer of size 20 each. Choose the correct input and output size suitable for the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-spanking",
   "metadata": {
    "id": "hollywood-spanking"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        # this line always has to be at the beginning\n",
    "        # of a new Module\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20,10)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # We need to bring the data into a format our\n",
    "        # neural network can handle. Try to understand\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        # Pass the input through your layer and add sigmoid activation function\n",
    "        X = self.fc1(X)\n",
    "        X = torch.sigmoid(X)\n",
    "        X = self.fc2(X)\n",
    "        X = torch.sigmoid(X)\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-appreciation",
   "metadata": {
    "id": "literary-appreciation"
   },
   "source": [
    "Instantiate the `NeuralNetwork`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-wheel",
   "metadata": {
    "id": "alpha-wheel"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-journalism",
   "metadata": {
    "id": "fleet-journalism"
   },
   "source": [
    "## 5. Set the hyperparameters and choose a suitable loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-monster",
   "metadata": {
    "id": "duplicate-monster"
   },
   "source": [
    "Instantiate an optimizer and a loss function. Use stochastic gradient descent as your optimizer and pick a suitable loss function for the data. You can look up how to create an optimizer [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-invitation",
   "metadata": {
    "id": "hispanic-invitation"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-drain",
   "metadata": {
    "id": "brown-drain"
   },
   "source": [
    "## 6. Create a training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-mitchell",
   "metadata": {
    "id": "operational-mitchell"
   },
   "source": [
    "The trainig loop should receive the `net`, `train_dl`, `val_dl`, `epochs`, `optimizer`, and `loss_func`.\n",
    "Print out the average loss and the accuarcy on both the train **and** valiadtion data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a-R4Up5UWIDS",
   "metadata": {
    "id": "a-R4Up5UWIDS"
   },
   "outputs": [],
   "source": [
    "# To get a better idea of how well your model performs\n",
    "# you should implement an accuracy function that is\n",
    "# called after each epoch of your training loop\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-ownership",
   "metadata": {
    "id": "broadband-ownership"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl , epochs, optimizer, loss_func):\n",
    "    print(\"epoch | train loss | train acc | val loss | val acc\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      # set model in training state\n",
    "      model.train()\n",
    "      total_acc = 0\n",
    "      total_loss = 0\n",
    "\n",
    "      for xb, yb in train_dl:\n",
    "\n",
    "        # make the prediction\n",
    "        pred = model(xb)\n",
    "        # calc the loss\n",
    "        loss = loss_func(pred, yb)\n",
    "        # zero the all gradients\n",
    "        # calc gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += accuracy(pred, yb)\n",
    "\n",
    "      total_loss /= len(train_dl)\n",
    "      total_acc /= len(train_dl)\n",
    "\n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        for xb_val, yb_val in val_dl:\n",
    "          pred_val = model(xb_val)\n",
    "          loss_val = loss_func(pred_val, yb_val)\n",
    "          total_loss_val += loss_val\n",
    "          total_acc_val += accuracy(pred_val, yb_val)\n",
    "\n",
    "\n",
    "      total_acc_val /= len(val_dl)\n",
    "      total_loss_val /= len(val_dl)\n",
    "      print(\"---------------------------------------------------\")\n",
    "      print(f\"  {epoch}   |    {total_loss.item():.4f}  |  {total_acc.item():.4f}   |  {total_loss_val.item():.4f}  |   {total_acc_val.item():.4f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-provision",
   "metadata": {
    "id": "decimal-provision"
   },
   "source": [
    "## 7. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MCm85TEETnGI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79732,
     "status": "ok",
     "timestamp": 1651701246475,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "MCm85TEETnGI",
    "outputId": "6ee31eb6-e0e3-4498-ec0e-0b814bb5dd66"
   },
   "outputs": [],
   "source": [
    "# Execute the train function and train the model.\n",
    "train(model, train_dl, val_dl, epochs, optimizer, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V7GB9aL-iXfH",
   "metadata": {
    "id": "V7GB9aL-iXfH"
   },
   "source": [
    "# 8. Check your test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KnufgGyAiayL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1651701247994,
     "user": {
      "displayName": "Johannes Melsbach",
      "userId": "07647488239628991813"
     },
     "user_tz": -120
    },
    "id": "KnufgGyAiayL",
    "outputId": "19384a6c-a896-4780-d30d-bc8644bdf81a"
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    # Perform a prediction on the test set\n",
    "    for xb_test, yb_test in test_dl:\n",
    "        model.eval()\n",
    "        pred_test = model(xb_test)  # Forward pass\n",
    "        acc = accuracy(pred_test, yb_test)\n",
    "        test_acc += acc\n",
    "\n",
    "    test_acc /= len(test_dl)\n",
    "\n",
    "print(f\"Test Accuracy is: {test_acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10m6PpYkS_ZLGjSMMUISSAg6qBfufb50-",
     "timestamp": 1620146133729
    },
    {
     "file_id": "1Fjshka73t0s-UsF6QTipbV0o8QtnLul4",
     "timestamp": 1619689907945
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
