{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8uk9HnwiUik"
   },
   "source": [
    "# Exercise 03 Notebook - Text Classification with BERT\n",
    "In this notebook we will create a Neural Network for Text Classification. As a basis for our Network we will use a pretrained BERT Model and just add a classification layer. We will train and test our dataset on the IMDB dataset we have already seen in the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88XYrS4ajzlP"
   },
   "source": [
    "## 1. Download the data, model , and tokenizer\n",
    "In a first step we will install transformers, an awesome library by huggingface that implements a lot of transformer models and also provides pretrained models in many languages that we can download and use for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvGYkUUJ5_VX"
   },
   "outputs": [],
   "source": [
    "# install transformer library via pip\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9448Mx8lCHT"
   },
   "source": [
    "Before we download a pretrained model, we download the data we will train our classifier on. Read the data from `https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oidZKgWlZxD"
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXJYl2l4IYCJ"
   },
   "outputs": [],
   "source": [
    "# Read the csv file and save it in a variable called data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv-I_pPPpVHW"
   },
   "outputs": [],
   "source": [
    "# After reading in the data, shuffle the rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeLOUOaWwb_0"
   },
   "source": [
    "If you forget how the dataset looks like to a little bit of of data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9BzebVVwrXW"
   },
   "outputs": [],
   "source": [
    "# Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HtLtRgapslR"
   },
   "source": [
    "A lot of errors can happen when you build your dataset and model for the first time. It neat little trick is to not use all of your data from the beginning. The IMDB dataset we use has 50,000 training examples. To see if everything works we dont need to use all of our data. A small subset is enough, so it might be useful to just take 1000 examples at first and only use the full data at the very end when we no that everything works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUfaquLPpls2"
   },
   "outputs": [],
   "source": [
    "# get the first 1000 rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJjGt00spNJy"
   },
   "outputs": [],
   "source": [
    "# Make an 80:20 split for training and validation data and save\n",
    "# them as train_df and val_df, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge5ie7hClmKu"
   },
   "source": [
    "Great! Now that we have our data at hand we will now download the pretrained BERT Model. To be more precise we will use a smaller version of BERT named DistilBERT which is a lot smaller than the normnal BERT model but has 95% of it's performance.\n",
    "\n",
    "\n",
    "\n",
    "> The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "A smaller model allows much faster training and it makes much sense to try out ideas with a smaller model, because we can iterate much faster with a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kd_Uo6r6VOI"
   },
   "outputs": [],
   "source": [
    "# import model and tokenizer classes\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5IbyGwAnpwI"
   },
   "source": [
    "The transformer library makes it very easy for us to download pretrained versions of a model and the corresponding tokenizer. You can find a pretrained model [here](https://huggingface.co/distilbert-base-uncased). You could try out more models.\n",
    "\n",
    "As already mentioned we want to use DistilBert for our classifier. Try to find a suitable model that we can use for our dataset and save it's name as a string in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s419l5b4oWIk"
   },
   "outputs": [],
   "source": [
    "# insert model name\n",
    "model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEWJsJbk6xeI"
   },
   "outputs": [],
   "source": [
    "bert_model = DistilBertModel.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cVcneG8ovIj"
   },
   "source": [
    "## 2. Tokenizer\n",
    "After successfully downloading the pretrained model and tokenizer will use the  tokenizer to tokenize our text data.**bold text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVB4CG02uXLa"
   },
   "source": [
    "### 2.1 Getting familiar with the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e2_5CVVrjCo"
   },
   "source": [
    "As a little warm-up lets do a few exercises on the example text given below. If you don't know what to do have a look at the [tokenizer documentation](https://huggingface.co/transformers/main_classes/tokenizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuMkIpAqrqq9"
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"Star wars made epic fantasy real. For a generation of people it has defined what the cinema experience is meant to be.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naSc8pEvN4jM"
   },
   "outputs": [],
   "source": [
    "# tokenize the example_text\n",
    "# you should receive a list ['Star', 'wars', 'made', ..., '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAOSti7SN0Bm"
   },
   "outputs": [],
   "source": [
    "# encode the example text.\n",
    "# you should receive a list of integers [101, 2537, 8755, ..., 102]\n",
    "# save the list into a variable encoded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgaMk6BJOCxG"
   },
   "outputs": [],
   "source": [
    "# use the tokenizer to decode the encoded_text\n",
    "# do you notice something?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZoBEIzXtYqk"
   },
   "outputs": [],
   "source": [
    "# use the tokenizer object as a function and use the\n",
    "# example_text as the input. What type of object do you receive?\n",
    "# What does each value mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNybb_B1ucqp"
   },
   "source": [
    "### 2.2 Tokenizing the data\n",
    "We are now ready to tokenize our test and validation dataset. Again use the tokenizer to create input_ids and attention_masks for the test and validation set. The tokenized text should have the following properties:\n",
    "* [CLS] and [SEP] token added\n",
    "* max length should be 128\n",
    "* texts with more than 128 tokens should be truncated\n",
    "* the tokenizer should return torch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJo5NL97XcMC"
   },
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the training text data and save the resulting dict\n",
    "# in a variable called tokenized_val_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KdLaFwRfP3O"
   },
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the validation text data and save the resulting dict\n",
    "# in a variable called tokenized_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzCZ8ypYXfMv"
   },
   "outputs": [],
   "source": [
    "# have a look at the input_ids of the tokenized_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynf_QqS7XwUb"
   },
   "outputs": [],
   "source": [
    "# have a look at the attention_mask of the tokenized_train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLmYtMOXwDIJ"
   },
   "source": [
    "## 3. Creating the test and validation Dataset and DataLoader\n",
    "Our train and evaluation data in vector format and we can now use the tokenized text to create our Dataset and DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTYWyiVBXOiG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import Dataset and DataLoader class\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMwTMKT7xLly"
   },
   "source": [
    "Now create a Dataset class called TextDataset. As always we will need to implement three functions:\n",
    "* `__init__`\n",
    "* `__len__`\n",
    "* `__getitem__`\n",
    "\n",
    "The `__init__` function should take tokenized data and the labels as arguments\n",
    "and store them into the class variables `X` and `Y`\n",
    "\n",
    "The `__len__` function should return the length of the dataset\n",
    "\n",
    "The `__getitem__` should take index as input and return a tuple of data that looks like this `(input_ids, attention_mask, labels)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMcFKxygZW93"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, Y):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def __len__(self,):\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5jr3430yX1P"
   },
   "source": [
    "Create a `train_dataset` and `val_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH4cudPNZXz3"
   },
   "outputs": [],
   "source": [
    "train_dataset = pass\n",
    "val_dataset = pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbuWY7dLyzVw"
   },
   "source": [
    "Create the training DataLoader `train_dl` and the validation DataLoader `val_dl` with a batch size of 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntTFu3O8yxzp"
   },
   "outputs": [],
   "source": [
    "train_dl = pass\n",
    "val_dl = pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rsG6VVzfpn"
   },
   "source": [
    "# 4. Creating the Model\n",
    "In this part we will create our model using the pretrained DistilBert model that we downloaded at the beginning. Before we add our classifier to the network we first need to understand what exactly the DistilBert models output looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HkrcYJEgQJ3"
   },
   "source": [
    "### 4.1 Understanding BERT's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwfDXGZr1xsE"
   },
   "outputs": [],
   "source": [
    "# get the first batch from our train_dl\n",
    "first_batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ool4ysw53TON"
   },
   "source": [
    "Have a look at the `first_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKcHyYbv5czd"
   },
   "outputs": [],
   "source": [
    "# first batch\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEewu3Pp3pbd"
   },
   "source": [
    "Save the input ids in a variable called `input_ids` and the attention mask into an variable called `attention_mask`. You can ignore the labels for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO53dijPgUsM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK6a6FIZ4Ewj"
   },
   "source": [
    "In the first chapter we downloaded the pretrained DistilBert Model and saved it as `bert_model`. For the forward propagation the `bert_model`expects input ids and attention masks as an input. [This blog](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) post is a very nice visualization and is very helpful for understanding coming out of the bert model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRgXKzKR2a0n"
   },
   "source": [
    "![Last hidden state](https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dwETN2r5YoK"
   },
   "outputs": [],
   "source": [
    "last_hidden_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjm08CGHekvq"
   },
   "source": [
    "Check the dimension of the models output and make sure you understand what each dimension represents. Slice the model so that you get all values for each `[CLS]` token in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hhTk801et9I"
   },
   "outputs": [],
   "source": [
    "# shape of last hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "th1yaUglfd9u"
   },
   "outputs": [],
   "source": [
    "# Select [CLS] Token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kUUvsvQf722"
   },
   "outputs": [],
   "source": [
    "# check the shape of the [CLS] tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Es53OwgZIt"
   },
   "source": [
    "### 4.2 Defining the Model\n",
    "Now create a neural network called `BertClassifier`. The constructor should receive the pretrained bert model and the number of classes.\n",
    "In the constructor save the bert model into a variable `bert`. Create a linear layer and think which input and output dimensions are needed.\n",
    "\n",
    "The `forward()` should receive `input_ids` and `attention_mask` as input and should propate them through the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eht1dRbBaZtA"
   },
   "outputs": [],
   "source": [
    "# implement BertClassifier\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, bert_model, n_classes):\n",
    "    pass\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmqxoH9bcU-X"
   },
   "outputs": [],
   "source": [
    "# instiantiate the model\n",
    "model = pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBS6Zo4ctfKC"
   },
   "source": [
    "## 5. Model Training\n",
    "After defining the model we now have everything we need to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywZHOUdHxA2a"
   },
   "source": [
    "### 5.1 Moving to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_79GWPUubFF"
   },
   "source": [
    "In previous exercises our models were quite small with only a couple of thousand parameters. Our BERT classifier is several magnitudes larger (about 66M parameters). With that many parameters it becomes necessary to train on the gpu.\n",
    "\n",
    "We can easily move our model to the gpu with the following command:\n",
    "  `model.to('gpu')`. But there is one problem. If there is no 'gpu' available the code will crash. There is an easy way to check if a gpu is available:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EId8J2Iev5tT"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__VP6sZ9wc67"
   },
   "source": [
    "We can now move our model to the gpu safely. If there is no gpu available the model just stays on the cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QACcTLD7cfPa"
   },
   "outputs": [],
   "source": [
    "# pass the model to the gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNOxzW_ewtAP"
   },
   "source": [
    "### 5.2 Setting up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebgzspzldym3"
   },
   "outputs": [],
   "source": [
    "epochs = pass\n",
    "lr = pass\n",
    "optimizer = pass\n",
    "loss_func = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wl2N3WAod6-P"
   },
   "outputs": [],
   "source": [
    "# To get a better idea of how well your model performs\n",
    "# you should implement an accuracy function that is\n",
    "# called after each epoch of your training loop\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYpQEvJcxK7z"
   },
   "outputs": [],
   "source": [
    "# Freezing Parameters of bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKFG6nNHxYG1"
   },
   "source": [
    "### 5.3 Train the model\n",
    "The training loop is almost the same as in the first exercise of the course. Spot and understand the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTt__Teupolv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueyyBXtbiOkE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFTFQrXahJ3u"
   },
   "outputs": [],
   "source": [
    "# Execute the train function and train the model.\n",
    "train(model, train_dl, val_dl, epochs, optimizer, loss_func)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
