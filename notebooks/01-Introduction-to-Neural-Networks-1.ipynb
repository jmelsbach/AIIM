{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import aiim\n",
    "    print('AIIM Library already installed.')\n",
    "except:\n",
    "    print('Installing and importing AIIM Library.')\n",
    "    %pip install git+https://github.com/jmelsbach/AIIM@main\n",
    "    import aiim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import fixed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Artificial Neuron\n",
    "In the following cell we will implement an Artificial Neuron which is the simplest form of a neural network. As we saw in the the lecture the formula of an Artificial Neuron is as follows:\n",
    "$$\\sum_i^n{w_ix_i + b = z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, n_inputs, n_outputs=1):\n",
    "    w = torch.randn(n_inputs, n_outputs) # weights\n",
    "    b = torch.randn(n_outputs) # biases\n",
    "    return x @ w + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the Artificial Neuron without a for loop by using matrix multiplication. We initialize the weights of our Neuron randomly. \n",
    "`torch.randn(n_inputs, n_outputs)` creates a random tensor of shape `n_inputs, n_outputs`. The number of biases matches the number of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an example input `x` and send put it in our Artificial Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(x, n_inputs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers\n",
    "You surely noticed that our code allows us to define more than one outputs using the `n_outputs` parameter. We can combine any input and outputsize!\n",
    "> ðŸ–‹ï¸  Try it yourself. Send our input `x` through our function. Chose arguments so that we have 3 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have wondered why we called our function `linear` and not `neuron`. Actually the function we created describes what is called a *Linear Layer* in *Deep Learning*. A *Neuron* is a special case of a *Linear Layer* where the number of outputs is `1`. A *Linear Layer* definitly deserves to have its own class so let's define it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "\n",
    "        self.w = torch.randn(in_features, out_features, requires_grad=True)\n",
    "        if bias:\n",
    "            self.b = torch.randn(out_features, requires_grad=True)\n",
    "        else:\n",
    "            self.b = 0\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the code we just wrote:\n",
    "`__init__` is the constructor of our class and allows us to set some parameters when we initially create our class.\n",
    "\n",
    "Our `forward` function does the actual calculations of our *Linear Layer* which is the same as in our `linear` function we previously wrote.\n",
    "\n",
    "The `__call__` function seems a little weird and unnecessary as it simply calls the `forward` function. Just as the `__init__` function it starts and ends with `__`. Those function are called *dunder functions* which stands for double underscore functions. There are several of those *dunder functions* in Python. The `__call__` function allows us to use our class object like a function which is very convinient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ–‹ï¸  Create a Linear Layer called `l1` with 3 inputs and 3 outputs and send `x` through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our object just like a function thanks to `__call__`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Linear Layers\n",
    "We have created a reusable class for a *Linear Layer* the next logical step is to stack this layers together.\n",
    "> ðŸ–‹ï¸  You can reuse the `l1` object from above. Create another layer called `l2` that has a single output and send `x` through both of them sequentially. How do we have to choose `n_inputs` of `l2`? Can we choose any value for it? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create l2\n",
    "\n",
    "# send x through l1 and then l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "* Create a class called `NeuralNetwork`. In the constructor define a `3x5` Linear Layer and a a second layer with one output.\n",
    "* Define a `__call__` function that calls a `forward` function.\n",
    "* Define a `forward` function that receives an input `x` and sends it through both *Linear Layers* and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NeuralNetwork class\n",
    "class NeuralNetwork:\n",
    "    pass # replace with your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send x through the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Linear Functions\n",
    "We successfully created our first neural network that consists of two layers. The truth is this was not a real *Neural Network*, yet. We combined two linear layers or linear functions, respectively. In the following we will look at a visual proof that stacking two (or more) linear functions into each other results in another linear function. So actually we gain nothing with an additional layer, we could have achieved the same result with a single linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from aiim.visualization import plot_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a simple linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x, m, b):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `partial` from the `functools` to fixate the parameters m and b for two linear functions. You can ignore the `.__name__` part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_function1 = partial(linear_function, m=2, b=0)\n",
    "linear_function2 = partial(linear_function, m=5, b=3)\n",
    "\n",
    "linear_function1.__name__ = 'linear_function1'\n",
    "linear_function2.__name__ = 'linear_function2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot `linear_function1` and `linear_function2` individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(linear_function1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(linear_function2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing surprising here. Let us now write another function that chains the two linear functions and plot the new `combined_linear_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_linear_function(x):\n",
    "    return linear_function2(linear_function1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(combined_linear_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting figure leaves no doubt that combining the two linear functions again results in just another linear function. We can even calculate `m` the slope and the intercept `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l2(l1(x)) = (x*2+0)*5 + 3  = 10x + 3$\n",
    "\n",
    "So the combination of the two linear functions resulted in yet another linear function with $m=10$ and $b=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions or Nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiim.visualization import interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to implement the tanh function yourself!\n",
    "\n",
    "$tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    pass # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_plot(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "The softmax function, often used in the final layer of a neural network classifier, converts logits (numeric output scores from the model) into probabilities by taking the exponential of each output and then normalizing these values by dividing by the sum of all the exponentials. This ensures that the output values are in the range (0, 1) and sum up to 1, making them interpretable as probabilities. The softmax function is particularly useful for multi-class classification problems.\n",
    "\n",
    "The formula for the softmax function for a vector $z$ of raw class scores from the final layer of a neural network is given by:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $z_i$ is the score for class $i$.\n",
    "- The denominator is the sum of the exponentials of all the raw class scores.\n",
    "\n",
    "This formula ensures that the scores are normalized and interpretable as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiim.visualization import softmax_with_sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_with_sliders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the softmax with the help of the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert softmax(torch.randn(5)).sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Network with PyTorch\n",
    "\n",
    "We have everything we need to create a forward pass with a Neural Network. In `Exercise 1` we implemented our own neural network but in practice it makes sense to use a framework like PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch everything resolves about the class `nn.Module`. Each Layer is an `nn.Module` and the Neural Network itself is also an `nn.Module`.\n",
    "\n",
    "We need two things.\n",
    "* An `__init__` function where you usually define the layers and that calls the constructor of the superclass `super().__init__` at the beginning.\n",
    "* A forward function that passes the input in the desired order through our Network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `nn` module you will find an implementation of the a Linear Layer called `nn.Linear` which basically works exactly as the your `Linear` class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create a two layer Neural Network that has 10 inputs and 5 outputs at the end. Use activations functions after each Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNeuralNetwork(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "In this section we want to have a look at two different loss functions, namely, `Cross Entropy Loss` and `Mean Squared Error` Loss. There are a lot more loss functions that exist and which one you use depends on the problem you want to solve. Sometimes you can use different loss functions for the same problem. For example you can use both the the `Cross Entropy Loss` and the `Mean Squared Error` Loss for a single-label classification with two classes. However, for each Loss functions expect the data in different formats and you as as the developer have to make sure it is in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss\n",
    "We discussed the MSE Loss in detail in the lecture. MSELoss is most of the time used when dealing with regression problems but can also been used for classification as stated above.\n",
    "\n",
    "Imagine you want to predict if a move review is positive or negative. We would design our neural network so that it has one output that should be close to `1` if the review is positiv, and close to `0` if it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an example output and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.randn(1)\n",
    "label = torch.randint(0,1,[1])\n",
    "prediction = logit.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have dummy values for our prediction as well as a label. Let's put this into our Cross Entropy Loss Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSELoss()(prediction, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "What would be the shape of the `prediction` and `label` if we had a batch size of `4`? Create an example for the `prediction` and the `label` for a batch size of `4` and calculate the MSELoss for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, if we run the code we get an `RuntimeError` telling us `Found dtype Long but expected Float`. What does that mean? Let's have a look at the datatype of our `label` and `prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "The Cross Entropy Loss is used for single label classification and is widely used to train neural networks. For example, Language Models like GPT predict the next word based on all previous words to generate language. GPT outputs one logit for each word in it's vocabulary and we applies the `softmax` to get the most likely next word, by assigning a probability to each word. During training the loss is calculated with the `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully read the documentation. It states that\n",
    "> The input is expected to contain the unnormalized logits for each class\n",
    "\n",
    "Take a pause and try to remember how we defined logits in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits are the raw output of the neural network. This means that we do not have to apply the `softmax` function as this is already done for you in the loss function!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some example data and have a look on how the data is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "logits = torch.randn(4, 5, requires_grad=True)\n",
    "target = torch.empty(4, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits are random positive and negative numbers and simulate the output of the last layer of a neural network. What does the shape `(4,5)` mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, target.shape, target.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is more interesting. It is a tensor of shape `[4]` with a dtype of `integer` with values between `0` and `4`. This integers encode the correct class of our single-label classification problem.\n",
    "\n",
    "Imagine you classifiying images of animals and have the following classes:\n",
    "    ```\n",
    "    {\n",
    "        0: 'cat',\n",
    "        1: 'dog',\n",
    "        2: 'horse',\n",
    "        3: 'duck',\n",
    "        4: 'monkey'\n",
    "    }\n",
    "    ```\n",
    "So your job as a developer is to make sure that your dataset returns a training example formated as `(image_as_tensor, label_as_int)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the loss value for our example batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropyLoss()(logits, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
